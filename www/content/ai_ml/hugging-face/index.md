---
title: "Huggingface Transformersã§BERTã‚’Fine Tuningã—ã¦ã¿ã‚‹"
date: "2021-05-05"
tags:
  - "AI/ML"
  - "NLP"
  - "Huggingface"
thumbnail: "ai_ml/hugging-face/images/thumbnail.png"
---
# Huggingface Transformersã§BERTã‚’Fine Tuningã—ã¦ã¿ã‚‹

## TL;DR

æ§˜ã€…ãªè‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã‚’ãŠæ‰‹è»½ã«ä½¿ãˆã‚‹[Huggingface Transformers](https://github.com/huggingface/transformers)ã‚’åˆ©ç”¨ã—ã€æ—¥æœ¬èªã®äº‹å‰å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«ã®Fine Tuningã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚

ä¾‹ã«ã‚ˆã£ã¦ã¯ãƒ†ã‚¹ãƒˆã§åˆ©ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[äº¬éƒ½å¤§å­¦æƒ…å ±å­¦ç ”ç©¶ç§‘--NTTã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç§‘å­¦åŸºç¤ç ”ç©¶æ‰€ å…±åŒç ”ç©¶ãƒ¦ãƒ‹ãƒƒãƒˆ](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/index.php)ãŒæä¾›ã™ã‚‹ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚

Transformers 4.5.1æ™‚ç‚¹ã§ã¯ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¾ã¾ã§ã¯ä¸Šæ‰‹ãã„ãªã‹ã£ãŸã®ã§æ³¨æ„äº‹é …ã‚’å«ã‚ã¦è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿

[äº¬éƒ½å¤§å­¦æƒ…å ±å­¦ç ”ç©¶ç§‘--NTTã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç§‘å­¦åŸºç¤ç ”ç©¶æ‰€ å…±åŒç ”ç©¶ãƒ¦ãƒ‹ãƒƒãƒˆ](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/index.php)ãŒæä¾›ã™ã‚‹ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã«å¯¾ã—ã¦ä»¥ä¸‹ã®4ã¤ã®åˆ†é¡ãŒã•ã‚Œã¦ã„ã¾ã™ã€‚

* ã‚°ãƒ«ãƒ¡
* æºå¸¯é›»è©±
* äº¬éƒ½
* ã‚¹ãƒãƒ¼ãƒ„

## äº‹å‰æº–å‚™

ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚

```python
!mkdir data
!wget http://nlp.ist.i.kyoto-u.ac.jp/kuntt/KNBC_v1.0_090925_utf8.tar.bz2 -O data/KNBC_v1.0_090925_utf8.tar.bz2
```

```python
%cd data
!tar xvf KNBC_v1.0_090925_utf8.tar.bz2
%cd ..
```

```python
import re

def get_sentences_from_text(filename):
  sentences = []
  with open(filename, 'r') as f:
    for i, line in enumerate(f):
      sentence = line.split('\t')[1].strip()
      if sentence == '': # ç©ºæ–‡å­—ã‚’é™¤å»ã€‚
        continue
      if re.match('^http.*$', sentence): # URLã‚’é™¤å»ã€‚
        continue
      sentences.append(sentence)
  return sentences
```

```python
import os
import pandas as pd

root_dir = 'data/KNBC_v1.0_090925_utf8/corpus2'
targets = ['Gourmet', 'Keitai', 'Kyoto', 'Sports']

original_data = []
for target in targets:
  filename = os.path.join(root_dir, f'{target}.tsv')
  sentences = get_sentences_from_text(filename)
  for sentence in sentences:
    original_data.append([target, sentence])

original_df = pd.DataFrame(original_data, columns=['target', 'sentence'])
```

```python
display(original_df.head())
display(original_df.tail())
display(pd.DataFrame(original_df['target'].value_counts()))
```

```python
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(original_df, test_size=0.1)
```

![](images/data-sample.png)

## ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰

Google Colabã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚

### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

Transformersã¨ä»Šå›ä½¿ç”¨ã™ã‚‹å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«ãŒè¦æ±‚ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```python
!pip install transformers
!pip install fugashi
!pip install ipadic
```

### ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰

```python
import pandas as pd

data_root = 'Your Own Drive/data'

train_df = pd.read_csv(f'{data_root}/knbc-train.csv', index_col=0)
test_df = pd.read_csv(f'{data_root}/knbc-test.csv', index_col=0)
```

### ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›

```python
from transformers import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
```

```python
train_encodings = tokenizer(train_df['sentence'].to_list(), truncation=True, padding=True)
test_encodings = tokenizer(test_df['sentence'].to_list(), truncation=True, padding=True)
```

```python
import tensorflow as tf

label2index = {
    'Kyoto': 0,
    'Keitai': 1,
    'Gourmet': 2,
    'Sports': 3,
}

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_df['target'].map(lambda x: label2index[x]).to_list()
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_df['target'].map(lambda x: label2index[x]).to_list()
))
```

### å­¦ç¿’ã®å®Ÿè¡Œ

TransformersãŒç”¨æ„ã™ã‚‹Trainer(TensorFlowç”¨ã¯TFTrainer)ã‚’åˆ©ç”¨ã—ã¦å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

`compute_metrics`ã¯å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è¿½åŠ ã®Metricsã‚’å–å¾—ã™ã‚‹ãŸã‚ã«è¨­å®šã—ã¦ã„ã¾ã™ã€‚
æ³¨æ„ã™ã¹ãã¯`model.classifier`ã®è¨­å®šã§ã™ã€‚å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šã¯ä¸è¦ãªã¯ãšã§ã™ãŒã€æå¤±é–¢æ•°ã§æœŸå¾…ã™ã‚‹æ´»æ€§åŒ–é–¢æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è¨­å®šã—ãªã„ã¨å­¦ç¿’ãŒé€²ã¿ã¾ã›ã‚“ã§ã—ãŸã€‚ä»Šå¾Œæ”¹å–„ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚ã€4.5.1æ™‚ç‚¹ã§ã®Tipsã¨ãªã‚Šã¾ã™ã€‚


```python
from transformers import TFTrainer, TFTrainingArguments, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import tensorflow as tf

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10
)

with training_args.strategy.scope():
    model = TFBertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
    # Transformers 4.5.1ã®TFBertForSequenceClassificationã§ã¯
    # classifierã«activationãŒè¨­å®šã•ã‚Œã¦ãŠã‚‰ãšã€å¤šé …åˆ†é¡ã§lossãŒè¨ˆç®—å‡ºæ¥ãªã„ãŸã‚ã€
    # å·®ã—æ›¿ãˆã¦ã„ã¾ã™ã€‚
    model.classifier = tf.keras.layers.Dense(
            units=4, # ã‚¯ãƒ©ã‚¹æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
            activation='softmax',
            name='classifier',
        )

trainer = TFTrainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset,             # evaluation dataset
    compute_metrics=compute_metrics
)

trainer.train()
```

### è©•ä¾¡

Trainerã®evaluateãƒ¡ã‚½ãƒƒãƒ‰ã§è©•ä¾¡ã‚’å®Ÿè¡Œå‡ºæ¥ã¾ã™ã€‚

```python
trainer.evaluate()
```

ä»¥ä¸‹ã¯å®Ÿè¡Œçµæœã®ä¾‹ã§ã™ã€‚

```
{'eval_accuracy': 0.8370535714285714,
 'eval_f1': 0.8375614865485349,
 'eval_loss': 0.4905424118041992,
 'eval_precision': 0.8416657684509984,
 'eval_recall': 0.8370535714285714}
```

ä»¥ä¸‹ã‚’å®Ÿè¡Œã™ã‚‹ã¨TensorBoardã§ã‚‚ç¢ºèªãŒã§ãã¾ã™ã€‚

```python
%load_ext tensorboard
%tensorboard --logdir logs
```

## ã¾ã¨ã‚

[Huggingface Transformers](https://github.com/huggingface/transformers)ã‚’ä½¿ã†äº‹ã§ã€ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®æº–å‚™ãŒä¸è¦ã«ãªã‚Šã€ã‹ãªã‚Šç°¡å˜ã«æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

Trainerã®ã‚ˆã†ã«TransfomersãŒç”¨æ„ã™ã‚‹æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã ã‘ã§ç²¾åº¦ã®ç¢ºèªã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã¨æ€ã„ã¾ã™ã®ã§ã€BERTä»¥å¤–ã«ALBERTã‚„T5ã¨ã®æ¯”è¼ƒãªã©ã‚‚è©¦ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

## å‚è€ƒæ–‡çŒ®

* [Huggingface - Training and fine-tuning](https://huggingface.co/transformers/training.html)
* [cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)
* [äº¬éƒ½å¤§å­¦æƒ…å ±å­¦ç ”ç©¶ç§‘--NTTã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç§‘å­¦åŸºç¤ç ”ç©¶æ‰€ å…±åŒç ”ç©¶ãƒ¦ãƒ‹ãƒƒãƒˆ](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/index.php)