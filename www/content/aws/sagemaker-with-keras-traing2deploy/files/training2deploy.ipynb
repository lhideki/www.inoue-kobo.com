{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMakerでKerasの独自モデルをトレーニングしてデプロイするまで(Python3対応)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "[AWS SageMaker](https://aws.amazon.com/jp/sagemaker/)において、Kerasによる独自モデルをトレーニングし、SageMakerのエンドポイントとしてデプロイします。\n",
    "また、形態素解析やベクトル化のような前処理を、個別にDockerコンテナを作成することなしにエンドポイント内で行うようにします。このために、[SageMaker TensorFlow Serving Container](https://github.com/aws/sagemaker-tensorflow-serving-container)を利用します。\n",
    "\n",
    "`SageMaker TensorFlow Serving Container`を利用するメリットは以下のとおりです。\n",
    "\n",
    "* 学習時はスクリプトモードでOK。\n",
    "* 形態素解析やベクトル化のような前処理を行う独自の推論用スクリプトを利用できる。\n",
    "* 個別にDockerコンテナを作成する必要が無い。\n",
    "* [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html?highlight=TensorFlowMOdel#tensorflow-model)と違ってPython3が利用できる。\n",
    "\n",
    "### KerasとTensorFlow付属のKerasの違いに注意\n",
    "\n",
    "Kerasは独立したモジュールとTensorFlow付属のモジュールの2種類があります。\n",
    "\n",
    "SageMakerのエンドポイント用のモデルは、Kerasのモデルではなく、TensorFlowのモデルとして保存する必要があり、Estimator内で実行する学習用コードの中でTensorFlowのモデルとしてKerasモデルを保存する必要があります。\n",
    "\n",
    "しかしながら、SageMakerのEstimatorがTensorFlowのセッションを初期化するため、独立したモジュールのKerasを利用するとセッション初期化のタイミングの問題により、Estimator内で実行する学習用コードの中でTensorFlowのモデルとして保存する場合に、未初期化の変数が発生して失敗します。\n",
    "\n",
    "このため、以下の様にTensorFlow付属のKerasモジュールのみを利用してKerasモデルを作成します。\n",
    "\n",
    "```python\n",
    "# 以下のように`keras`を利用することはできません。\n",
    "# from keras import Input, Model\n",
    "# from keras.layers.wrappers import Bidirectional\n",
    "# from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "\n",
    "# 以下のように`tensorflow.python.keras`を使用します。\n",
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras.layers.wrappers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "```\n",
    "\n",
    "### ラベルデータの準備\n",
    "\n",
    "ラベルデータはAWS S3上に配置されていることが前提となっています。\n",
    "\n",
    "### ベンチマーク用データ\n",
    "\n",
    "[京都大学情報学研究科--NTTコミュニケーション科学基礎研究所 共同研究ユニット](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/index.php)が提供するブログの記事に関するデータセットを利用しました。 このデータセットでは、ブログの記事に対して以下の4つの分類がされています。\n",
    "\n",
    "* グルメ\n",
    "* 携帯電話\n",
    "* 京都\n",
    "* スポーツ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全体の流れ\n",
    "\n",
    "* 事前準備\n",
    "* 学習用スクリプトとモジュールの準備\n",
    "* トレーニングの実行\n",
    "* 学習したモデルのダウンロード\n",
    "* 推論用スクリプトを含めてパッケージング\n",
    "* エンドポイントの作成\n",
    "* エンドポイントの削除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備\n",
    "\n",
    "### 学習用スクリプトの準備\n",
    "\n",
    "`train/train_v1.py`として用意します。\n",
    "\n",
    "前述のようにTensorFlow付属のKerasモジュールのみを利用してモデルを作成する必要があります。\n",
    "また、モデルの保存はTensorFlowのモデルとして保存するために`tf.saved_model.simple_save`を使用します。\n",
    "\n",
    "その他、Kerasに依存したサードパーティーモジュールを利用する場合、それらのサードパーティーモジュールもTensorFlow付属のKerasを利用する必要があります。\n",
    "今回は`keras-self-attention`を利用するため、環境変数として`TF_KERAS`に`1`を設定してから`import`することで、`keras-self-attention`がTensorFlow付属のKerasを利用するようになります(これは`keras-self-attention`独自の実装です)。\n",
    "\n",
    "### 推論用スクリプトの準備\n",
    "\n",
    "`predict/code/inference.py`として用意します。この際、`code/inference.py`は固定である点に注意してください。\n",
    "`SageMaker TensorFlow Serving Container`は、以下のように推論するための学習済みモデルと推論用スクリプトが並んで存在することを前提としています。\n",
    "\n",
    "```\n",
    "model.tar.gz\n",
    "├── [Model Version]\n",
    "│   ├── variables\n",
    "│   └── saved_model.pb\n",
    "└── code\n",
    "    ├── inference.py\n",
    "    └── requirements.txt\n",
    "\n",
    "```\n",
    "\n",
    "学習時と違い、`code`配下に`requirements.txt`を配置することで、デプロイ時に依存したモジュールを自動的にインストールしてくれます。\n",
    "\n",
    "### ディレクトリ構成\n",
    "\n",
    "```\n",
    "├── predict\n",
    "│   ├── [Model Version] <- ダウンロードした学習済みモデル。\n",
    "│   └── code\n",
    "│       ├── inference.py\n",
    "│       └── requirements.txt\n",
    "└── train\n",
    "    ├── train_v1.py\n",
    "    └── [Moduels] <- 学習用スクリプトが依存するモジュール。\n",
    "```\n",
    "\n",
    "`train/[Modules]`は学習用スクリプトと同じディレクトリに配置することで、学習用スクリプトの方でパスの違いを意識せずに`import`することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用スクリプトとモジュールの準備\n",
    "\n",
    "学習用スクリプトと依存するモジュールを準備します。\n",
    "依存するモジュールは`pip install [Module] --target train`で対象のディレクトリに配置します。\n",
    "\n",
    "Estimatorでは、この`train`配下を`src`に指定することで、学習で使用するコンテナにまるごと配置します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:23:09.735531Z",
     "start_time": "2019-07-14T02:20:07.049616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install text-vectorian --target train\n",
    "!pip install keras-self-attention --target train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:23:09.864330Z",
     "start_time": "2019-07-14T02:23:09.738948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "# SeqSelfAttentionをTensorFlow付属のKerasモジュールを利用するために必要な設定です。\n",
      "# 以下の設定が無効な場合は、Kerasのバージョンが合わずに_create_modelが失敗します。\n",
      "os.environ['TF_KERAS'] = '1'\n",
      "\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras_self_attention import SeqSelfAttention\n",
      "import pickle\n",
      "import logging\n",
      "from tensorflow.python.keras import utils\n",
      "from tensorflow.python.keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint\n",
      "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
      "from tensorflow.python.keras import Input, Model\n",
      "from tensorflow.python.keras.layers.wrappers import Bidirectional\n",
      "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Embedding\n",
      "from tensorflow.python.keras.models import Sequential, load_model\n",
      "from tensorflow.python.keras import backend as K\n",
      "import tensorflow as tf\n",
      "import argparse\n",
      "from text_vectorian import SentencePieceVectorian\n",
      "\n",
      "input_len = 64\n",
      "logger = logging.getLogger(__name__)\n",
      "vectorian = SentencePieceVectorian()\n",
      "\n",
      "\n",
      "def _save_model(checkpoint_filename, history, model_dir, output_dir):\n",
      "    '''\n",
      "    モデルや実行履歴を保存します。\n",
      "    '''\n",
      "    # Checkpointからモデルをロードし直し(ベストなモデルのロード)\n",
      "    model = load_model(checkpoint_filename,\n",
      "                       custom_objects=SeqSelfAttention.get_custom_objects())\n",
      "    # Historyの保存\n",
      "    history_df = pd.DataFrame(history.history)\n",
      "    history_df.to_csv(output_dir + f'/history.csv')\n",
      "\n",
      "    # Endpoint用のモデル保存\n",
      "    with tf.keras.backend.get_session() as sess:\n",
      "        tf.saved_model.simple_save(\n",
      "            sess,\n",
      "            args.model_dir + '/1',\n",
      "            inputs={'inputs': model.input},\n",
      "            outputs={t.name: t for t in model.outputs})\n",
      "\n",
      "\n",
      "def _save_labels(data, output_dir):\n",
      "    '''\n",
      "    推論で利用するためのラベルデータの情報を保存します。\n",
      "    '''\n",
      "\n",
      "    del data['labels']\n",
      "    del data['features']\n",
      "\n",
      "    data_filename = output_dir + f'/labels.pickle'\n",
      "\n",
      "    with open(data_filename, mode='wb')as f:\n",
      "        pickle.dump(data, f)\n",
      "\n",
      "\n",
      "def _load_labeldata(train_dir):\n",
      "    '''\n",
      "    ラベルデータをロードします。\n",
      "    '''\n",
      "    features_df = pd.read_csv(\n",
      "        f'{train_dir}/features.csv', dtype=str)\n",
      "    labels_df = pd.read_csv(f'{train_dir}/labels.csv', dtype=str)\n",
      "    label2index = {k: i for i, k in enumerate(labels_df['label'].unique())}\n",
      "    index2label = {i: k for i, k in enumerate(labels_df['label'].unique())}\n",
      "    class_count = len(label2index)\n",
      "    labels = utils.np_utils.to_categorical(\n",
      "        [label2index[label] for label in labels_df['label']], num_classes=class_count)\n",
      "\n",
      "    features = []\n",
      "\n",
      "    for feature in features_df['feature']:\n",
      "        features.append(vectorian.fit(feature).indices)\n",
      "    features = pad_sequences(features, maxlen=input_len, dtype='float32')\n",
      "\n",
      "    logger.info(\n",
      "        f'データ数: {len(features_df)}, ラベル数: {class_count}, Labels Shape: {labels.shape}, Features Shape: {features.shape}')\n",
      "\n",
      "    return {\n",
      "        'class_count': class_count,\n",
      "        'label2index': label2index,\n",
      "        'index2label': index2label,\n",
      "        'labels': labels,\n",
      "        'features': features,\n",
      "        'input_len': input_len\n",
      "    }\n",
      "\n",
      "\n",
      "def _create_model(input_shape, hidden, class_count):\n",
      "    '''\n",
      "    モデルの構造を定義します。\n",
      "    '''\n",
      "\n",
      "    wv = vectorian._vectorizer.model.wv\n",
      "    input_tensor = Input(input_shape)\n",
      "\n",
      "    # TensorFlow付属のKerasモジュールを使用する必要があるため、\n",
      "    # gensimのget_keras_embeddingを利用せずに、kerasのEmbeddingレイヤーを\n",
      "    # 直接使用します。\n",
      "    x1 = Embedding(input_dim=wv.vectors.shape[0], output_dim=wv.vectors.shape[1], weights=[wv.vectors], trainable=False)(input_tensor)\n",
      "    x1 = SeqSelfAttention(\n",
      "        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL)(x1)\n",
      "    x1 = Bidirectional(LSTM(hidden))(x1)\n",
      "    x1 = Dense(hidden * 2)(x1)\n",
      "    x1 = Dropout(0.1)(x1)\n",
      "    output_tensor = Dense(class_count, activation='softmax')(x1)\n",
      "\n",
      "    model = Model(input_tensor, output_tensor)\n",
      "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=[\n",
      "                  'acc', 'mse', 'mae', 'top_k_categorical_accuracy'])\n",
      "    model.summary()\n",
      "\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # Estimatorのパラメータは本スクリプトの引数として指定されるため、argparseで解析します。\n",
      "    # Pythonの文法上Estimatorのパラメータはスネークケースですが、引数として渡される場合は`--`をプレフィクスにしたチェーンケースになっています。\n",
      "    # なおhyperparameterとして渡すdictオブジェクトは、名前の変換は行われないのでそのまま引数名として受け取ります。\n",
      "    parser = argparse.ArgumentParser()\n",
      "    # Estimatorのパラメータとして渡されるパラメータ\n",
      "    parser.add_argument('--epochs', type=int, default=10)\n",
      "    parser.add_argument('--batch-size', type=int, default=100)\n",
      "    parser.add_argument('--hidden', type=int)\n",
      "    parser.add_argument('--container-log-level',\n",
      "                        type=int, default=logging.INFO)\n",
      "    parser.add_argument('--validation-split', type=float, default=0.1)\n",
      "    # 環境変数として渡されるパラメータ\n",
      "    parser.add_argument('--model-dir', type=str,\n",
      "                        default=os.environ['SM_MODEL_DIR'])\n",
      "    parser.add_argument('--train-dir', type=str,\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\n",
      "    parser.add_argument('--test-dir', type=str,\n",
      "                        default=os.environ['SM_CHANNEL_TEST'])\n",
      "    parser.add_argument('--output-dir', type=str,\n",
      "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
      "    parser.add_argument('--model-version', type=str,\n",
      "                        default='')\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "    # ログレベルを引数で渡されたコンテナのログレベルと合わせます。\n",
      "    logging.basicConfig(level=args.container_log_level)\n",
      "\n",
      "    # ラベルデータをロードします。\n",
      "    data = _load_labeldata(args.train_dir)\n",
      "    # モデルの定義を行います。\n",
      "    model = _create_model(data['features'][0].shape,\n",
      "                          args.hidden,\n",
      "                          data['class_count'])\n",
      "    # 学習用のデータを準備します。\n",
      "    train_features = data['features']\n",
      "    train_labels = data['labels']\n",
      "    # 学習を実行します。\n",
      "    # verboseを2に指定するのはポイントです。デフォルトは1ですが、そのままではプログレッシブバーの出力毎にログが記録されるため冗長です。\n",
      "    # 2にすることで、epochごとの結果だけ出力されるようになります。\n",
      "    if args.validation_split > 0:\n",
      "        monitor_target = 'val_acc'\n",
      "    else:\n",
      "        monitor_target = 'acc'\n",
      "\n",
      "    checkpoint_filename = f'model_{args.model_version}.h5'\n",
      "    history = model.fit(train_features, train_labels,\n",
      "                        batch_size=args.batch_size,\n",
      "                        validation_split=args.validation_split,\n",
      "                        epochs=args.epochs,\n",
      "                        verbose=2,\n",
      "                        callbacks=[\n",
      "                            EarlyStopping(\n",
      "                                patience=3, monitor=monitor_target, mode='max'),\n",
      "                            ModelCheckpoint(filepath=checkpoint_filename,\n",
      "                                            save_best_only=True, monitor=monitor_target, mode='max')\n",
      "                        ])\n",
      "\n",
      "    # Checkpointからロードし直すため、一度モデルを削除します。\n",
      "    del model\n",
      "    # 学習したモデルを保存します。\n",
      "    _save_model(checkpoint_filename, history, args.model_dir, args.output_dir)\n",
      "    # 推論時に利用するラベルデータの情報を保存します。\n",
      "    _save_labels(data, args.output_dir)\n"
     ]
    }
   ],
   "source": [
    "!cat train/train_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングの実行\n",
    "\n",
    "Estimatorを利用してトレーニングを実行します。\n",
    "\n",
    "なお、ハイパーパラメータのチューニングについては[SageMakerでTensorFlow+Kerasによる独自モデルをトレーニングする方法](https://www.inoue-kobo.com/aws/sagemaker-with-mymodel/index.html#hyperparametertuner)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:23:09.871187Z",
     "start_time": "2019-07-14T02:23:09.866384Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'sagemaker-with-keras-traing2deploy'\n",
    "TAGS = [{ 'Key': 'inoue-kobo.ProjectName', 'Value': PROJECT_NAME }]\n",
    "VERSION = 'v1'\n",
    "BUCKET_NAME = f'sagemaker-us-east-1.inoue-kobo.com'\n",
    "DATA_ROOT = f's3://{BUCKET_NAME}/{PROJECT_NAME}'\n",
    "TRAINS_DIR = f'{DATA_ROOT}/data/trains'\n",
    "TESTS_DIR = f'{DATA_ROOT}/data/tests'\n",
    "OUTPUTS_DIR = f'{DATA_ROOT}/outputs'\n",
    "ROLE = 'arn:aws:iam::489378379658:role/service-role/AmazonSageMaker-ExecutionRole-20181129T043923'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:23:12.376419Z",
     "start_time": "2019-07-14T02:23:11.126479Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import logging\n",
    "\n",
    "params = {\n",
    "    'batch-size': 256,\n",
    "    'epochs': 10,\n",
    "    'hidden': 32,\n",
    "    'validation-split': 0.1,\n",
    "    'model_version': VERSION\n",
    "}\n",
    "metric_definitions = [\n",
    "    {'Name': 'train:acc', 'Regex': 'acc: (\\S+)'},\n",
    "    {'Name': 'train:mse', 'Regex': 'mean_squared_error: (\\S+)'},\n",
    "    {'Name': 'train:mae', 'Regex': 'mean_absolute_error: (\\S+)'},\n",
    "    {'Name': 'train:top-k', 'Regex': 'top_k_categorical_accuracy: (\\S+)'},\n",
    "    {'Name': 'valid:acc', 'Regex': 'val_acc: (\\S+)'},\n",
    "    {'Name': 'valid:mse', 'Regex': 'val_mean_squared_error: (\\S+)'},\n",
    "    {'Name': 'valid:mae', 'Regex': 'val_mean_absolute_error: (\\S+)'},\n",
    "    {'Name': 'valid:top-k', 'Regex': 'val_top_k_categorical_accuracy: (\\S+)'},\n",
    "]\n",
    "estimator = TensorFlow(\n",
    "    role=ROLE,\n",
    "    source_dir='train',\n",
    "    entry_point=f'train_{VERSION}.py',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    framework_version='1.12.0',\n",
    "    py_version='py3',\n",
    "    script_mode=True,\n",
    "    hyperparameters=params,\n",
    "    output_path=OUTPUTS_DIR,\n",
    "    container_log_level=logging.INFO,\n",
    "    metric_definitions=metric_definitions,\n",
    "    tags=TAGS\n",
    ")\n",
    "inputs = {'train': TRAINS_DIR, 'test': TESTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T02:32:15.515319Z",
     "start_time": "2019-07-14T02:23:12.377804Z"
    }
   },
   "outputs": [],
   "source": [
    "import shortuuid\n",
    "\n",
    "uuid = shortuuid.ShortUUID().random(length=8)\n",
    "estimator.fit(job_name=f'{PROJECT_NAME}-{VERSION}-s-{uuid}', inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習したモデルのダウンロード\n",
    "\n",
    "`SageMaker TensorFlow Serving Container`では、推論用スクリプトと学習したモデルを一緒に含めて`model.tar.gz`として再パッケージする必要があります。\n",
    "このため、まずは学習済みモデルをダウンロードします。\n",
    "\n",
    "また、推論時に人が読んで理解できるラベルとして推論結果を出力するために、学習時に保存しておいた`labels.pickle`を含む`output.tar.gz`もダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import urllib\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "model_url = urllib.parse.urlparse(estimator.model_data)\n",
    "output_url = urllib.parse.urlparse(f'{estimator.output_path}/{estimator.latest_training_job.job_name}/output/output.tar.gz')\n",
    "\n",
    "bucket.download_file(model_url.path[1:], 'predict/model.tar.gz')\n",
    "bucket.download_file(output_url.path[1:], 'predict/output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/\n",
      "1/variables/\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/variables/variables.index\n",
      "1/saved_model.pb\n",
      "history.csv\n",
      "labels.pickle\n"
     ]
    }
   ],
   "source": [
    "!cd predict; tar zxvf model.tar.gz\n",
    "!cd predict; tar zxvf output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ダウンロードしたモデルの動作確認\n",
    "\n",
    "ダウンロードしたモデルをTensorflowで直接推論して動作を確認します。\n",
    "この手順は実施しなくても問題ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# TensorFlowによるモデルのロード\n",
    "session = tf.keras.backend.get_session()\n",
    "tf_model = tf.saved_model.loader.load(session, [tag_constants.SERVING], 'predict/1');\n",
    "\n",
    "# input/outputのシグネチャ名確認\n",
    "model_signature = tf_model.signature_def['serving_default']\n",
    "input_signature = model_signature.inputs\n",
    "output_signature = model_signature.outputs\n",
    "\n",
    "for k in input_signature.keys():\n",
    "    print(k)\n",
    "for k in output_signature.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input/output Tensorの取得\n",
    "input_tensor_name = input_signature['inputs'].name\n",
    "label_tensor_name = output_signature['dense_1_1/Softmax:0'].name\n",
    "\n",
    "input_name = session.graph.get_tensor_by_name(input_tensor_name)\n",
    "label_name = session.graph.get_tensor_by_name(label_tensor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論の実行\n",
    "import numpy as np\n",
    "from text_vectorian import SentencePieceVectorian\n",
    "\n",
    "vectorian = SentencePieceVectorian()\n",
    "max_len = 64\n",
    "features = np.zeros((1, max_len))\n",
    "inputs = vectorian.fit('これはグルメです。').indices\n",
    "\n",
    "for i, index in enumerate(inputs):\n",
    "    pos = max_len - len(inputs) + i\n",
    "    features[0, pos] = index\n",
    "\n",
    "label_pred = session.run([label_name], feed_dict={input_name: features})\n",
    "label_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論用スクリプトを含めてパッケージング\n",
    "\n",
    "推論用スクリプトと学習したモデルを一緒に`model.tar.gz`としてパッケージングします。\n",
    "また、推論結果を人が読んで理解できるラベルにマッピングするためdictである`labels.pickle`も`codeディレクトリ`に含めるようにすることで、推論用スクリプトから参照できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import io\n",
      "import json\n",
      "import requests\n",
      "import logging\n",
      "import numpy as np\n",
      "import pickle\n",
      "import pandas as pd\n",
      "from text_vectorian import SentencePieceVectorian\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "vectorian = SentencePieceVectorian()\n",
      "input_len = 64\n",
      "dim = 100\n",
      "\n",
      "def handler(data, context):\n",
      "    \"\"\"Handle request.\n",
      "    Args:\n",
      "        data (obj): the request data\n",
      "        context (Context): an object containing request and configuration details\n",
      "    Returns:\n",
      "        (bytes, string): data to return to client, (optional) response content type\n",
      "    \"\"\"\n",
      "    processed_input = _process_input(data, context)\n",
      "    response = requests.post(context.rest_uri, data=processed_input)\n",
      "\n",
      "    return _process_output(response, context)\n",
      "\n",
      "\n",
      "def _process_input(data, context):\n",
      "    if context.request_content_type == 'application/json':\n",
      "        body = data.read().decode('utf-8')\n",
      "\n",
      "        param = json.loads(body)\n",
      "        query = param['q']\n",
      "        features = np.zeros((1, input_len))\n",
      "        inputs = vectorian.fit(query).indices\n",
      "\n",
      "        for i, index in enumerate(inputs):\n",
      "            if i >= input_len:\n",
      "                break\n",
      "            pos = input_len - len(inputs) + i\n",
      "            features[0, pos] = index\n",
      "    \n",
      "        return json.dumps({\n",
      "            'inputs': features.tolist()\n",
      "        })\n",
      "\n",
      "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
      "        context.request_content_type or \"unknown\"))\n",
      "\n",
      "\n",
      "def _process_output(data, context):\n",
      "    if data.status_code != 200:\n",
      "        raise ValueError(data.content.decode('utf-8'))\n",
      "\n",
      "    response_content_type = 'application/json'\n",
      "\n",
      "    body = json.loads(data.content.decode('utf-8'))\n",
      "    predicts = body['outputs'][0]\n",
      "\n",
      "    labels_path = '/opt/ml/model/code/labels.pickle'\n",
      "\n",
      "    with open(labels_path, mode='rb') as f:\n",
      "        labels = pickle.load(f)\n",
      "    rets = _create_response(predicts, labels)\n",
      "\n",
      "    logger.warn(rets)\n",
      "\n",
      "    return json.dumps(rets), response_content_type\n",
      "\n",
      "def _create_response(predicts, labels):\n",
      "    rets = []\n",
      "\n",
      "    for index in np.argsort(predicts)[::-1]:\n",
      "        label = labels['index2label'][index]\n",
      "        prob = predicts[index]\n",
      "        rets.append({\n",
      "            'label': label,\n",
      "            'prob': prob\n",
      "        })\n",
      "\n",
      "    return rets\n"
     ]
    }
   ],
   "source": [
    "!cat predict/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/\n",
      "1/variables/\n",
      "1/variables/variables.index\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/saved_model.pb\n",
      "code/\n",
      "code/inference.py\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/requirements-checkpoint.txt\n",
      "code/.ipynb_checkpoints/inference-checkpoint.py\n",
      "code/labels.pickle\n",
      "code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!cd predict; mv labels.pickle code\n",
    "!cd predict; tar zcvf model.tar.gz 1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "predict_model_url = urllib.parse.urlparse(f'{estimator.output_path}/{estimator.latest_training_job.job_name}/predict/model.tar.gz')\n",
    "bucket.upload_file('predict/model.tar.gz', predict_model_url.path[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの作成\n",
    "\n",
    "`sagemaker.tensorflow.serving.Model`を利用してdeployを行います。\n",
    "この際、以下の注意事項があります。\n",
    "\n",
    "### frame_versionのバージョンによってはPythonの`f-string`が使えない\n",
    "\n",
    "`framework_version`に`1.13`を指定します。`1.12`だとPythonのバージョンが`3.5`であり`f-string`が使えないため要注意です。\n",
    "\n",
    "### インスタンスタイプの指定\n",
    "\n",
    "`ml.t2.mideum`ではメモリ不足で起動しなかったため、`ml.t2.large`にしています。\n",
    "なお、2019/07時点ではデプロイ時にt3系インスタンスを指定することができません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "tensorflow_serving_model = Model(model_data=f'{predict_model_url.scheme}://{predict_model_url.hostname}{predict_model_url.path}',\n",
    "                                 role=ROLE,\n",
    "                                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tensorflow_serving_model.deploy(initial_instance_count=1,\n",
    "                                            instance_type='ml.t2.large',\n",
    "                                            tags=TAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンドポイントによる推論結果確認\n",
    "\n",
    "boto3を使用してエンドポイントに文字列を入力することで、意図した推論結果が得られることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '携帯電話', 'prob': 0.96622}, {'label': '京都', 'prob': 0.0231401}, {'label': 'グルメ', 'prob': 0.00758497}, {'label': 'スポーツ', 'prob': 0.00305547}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "query = {\n",
    "    'q': '電波が悪い'\n",
    "}\n",
    "res = client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    Body=json.dumps(query),\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json'\n",
    ")\n",
    "body = res['Body']\n",
    "ret = json.load(body)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "最後に不要になったエンドポイントを削除します。\n",
    "エンドポイントの利用を継続する場合は、実施不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "* [SageMaker TensorFlow Serving Container](https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/README.md)\n",
    "* [SageMakerでTensorFlow+Kerasによる独自モデルをトレーニングする方法](https://www.inoue-kobo.com/aws/sagemaker-with-mymodel/index.html#hyperparametertuner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
