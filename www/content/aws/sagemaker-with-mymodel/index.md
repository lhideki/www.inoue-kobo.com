---
title: "SageMakerã§TensorFlow+Kerasã«ã‚ˆã‚‹ç‹¬è‡ªãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•"
date: "2019-01-20"
tags:
  - "AWS"
  - "SageMaker"
  - "TensorFlow"
  - "AI/ML"
thumbnail: "aws/sagemaker-with-mymodel/images/thumbnail.png"
---
# SageMakerã§TensorFlow+Kerasã«ã‚ˆã‚‹ç‹¬è‡ªãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•

## TL;DR

`AWS SageMaker`ã«ãŠã„ã¦ã€`TensorFlow+Keras`ã§ä½œæˆã—ãŸç‹¬è‡ªãƒ¢ãƒ‡ãƒ«ã‚’[Script Mode](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#preparing-a-script-mode-training-script)ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ç”¨ã®Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã«ã¤ã„ã¦ã¯SageMakerãŒæä¾›ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™ã€‚ã“ã®ãŸã‚ã€ç‹¬è‡ªã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ã•ã‚‰ã«ã€SageMakerçµ„ã¿è¾¼ã¿ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´æ©Ÿèƒ½ã‚’åˆ©ç”¨ã—ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•çš„ãªæ¢ç´¢ã‚’è¡Œã„ã¾ã™ã€‚
ãªãŠã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆã¯è¡Œã„ã¾ã›ã‚“ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯SageMakerã®æ©Ÿèƒ½ã¨ã—ã¦ã§ã¯ç„¡ãã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç›´æ¥åˆ©ç”¨ã—ã¾ã™ã€‚

### SageMakerã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã¨ã¯

SageMakerã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã§ã¯ã€Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨å­¦ç¿’ã®å®Ÿè¡Œã‚’è¨˜è¿°ã—ã€ãã‚Œã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ç”¨ã®Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦è¨­å®šã™ã‚‹äº‹ã§ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç«‹ã¡ä¸Šã’ã‹ã‚‰ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œã€çµæœã®ä¿å­˜ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®åœæ­¢ã¾ã§ã‚’ä¸€æ‹¬ã§ç®¡ç†ã—ã¦ãã‚Œã¾ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹äº‹ã§ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯å°ã•ãã¦å®‰ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ©ç”¨ã—ã€å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã®ã¿GPUã‚’åˆ©ç”¨ã—ãŸé«˜ä¾¡ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œã«ã¤ã„ã¦ã¯ã€SageMakerç”¨ã®AWSã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰è¡Œã†äº‹ãŒå¯èƒ½ã§ã™ã€‚

### ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯AWS S3ä¸Šã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå‰æã¨ãªã£ã¦ã„ã¾ã™ã€‚

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿

[äº¬éƒ½å¤§å­¦æƒ…å ±å­¦ç ”ç©¶ç§‘--NTTã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç§‘å­¦åŸºç¤ç ”ç©¶æ‰€ å…±åŒç ”ç©¶ãƒ¦ãƒ‹ãƒƒãƒˆ](http://nlp.ist.i.kyoto-u.ac.jp/kuntt/index.php)ãŒæä¾›ã™ã‚‹ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã«å¯¾ã—ã¦ä»¥ä¸‹ã®4ã¤ã®åˆ†é¡ãŒã•ã‚Œã¦ã„ã¾ã™ã€‚

* ã‚°ãƒ«ãƒ¡
* æºå¸¯é›»è©±
* äº¬éƒ½
* ã‚¹ãƒãƒ¼ãƒ„

## å…¨ä½“ã®æµã‚Œ

* ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èª¬æ˜
* å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
* Estimatorã®æº–å‚™
* ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
* ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´ã®å®Ÿè¡Œ
* è‡ªå‹•èª¿æ•´çµæœã®ç¢ºèª
* æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
* æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«–çµæœç¢ºèª

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èª¬æ˜

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆå ´æ‰€ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã«åˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã“ã“ã§ã¯`jobs`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«`train.py`ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«åã§ä½œæˆã—ã¦ã„ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—éƒ¨åˆ†ãŒã‚ã‚Šã¾ã™ãŒã€åŸºæœ¬çš„ã«æ™®é€šã«`keras`ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¦å­¦ç¿’ã•ã›ã‚‹ã®ã¨å¤‰ã‚ã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é…ç½®æ–¹æ³•

Script Modeã§ã¯å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’requirements.txtã§æŒ‡å®šã™ã‚‹ã®ã§ã¯ç„¡ãã€Estimatorã®`dependencies`ã«æŒ‡å®šã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«é…ç½®ã—ã¾ã™ã€‚
pipã§installã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®æ§˜ã«`taregetã‚ªãƒ—ã‚·ãƒ§ãƒ³`ã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é…ç½®å…ˆã‚’æŒ‡å®šã—ã¾ã™ã€‚

```bash
pip install [module] --target [dependenciesã«æŒ‡å®šã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª]
```

ã“ã“ã§ã¯`jobs/modules`ã«é…ç½®ã—ã¾ã™ã€‚`text-vectorian`ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œä¾‹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

```bash
pip install text-vectorian --target jobs/modules
```

### ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®é…ç½®å…ˆ

SageMakerã«ã‚ˆã‚ŠS3ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä¸Šã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
ç‰¹ã«æ„è­˜ã›ãšã«`SM_CHANNEL_*`ã§æ¸¡ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ãªãŠã€Pipe Modeã¨ã—ã¦S3ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ãªãŒã‚‰å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å†…å®¹

```bash
!cat jobs/train.py
```

```python
import os
import argparse
import pandas as pd
import numpy as np
import sys

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é…ç½®å…ˆã‚’èª­ã¿è¾¼ã¿å¯¾è±¡ã¨ã—ã¦æŒ‡å®šã—ã¾ã™ã€‚
# ã“ã‚Œã«ã‚ˆã‚Šé€šå¸¸ã¨åŒã˜ã‚ˆã†ã«importæŒ‡å®šã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚
sys.path.append('modules')

import keras
from keras.layers import Dense, Dropout, LSTM, Embedding, Reshape, RepeatVector, Permute, Flatten, Conv1D
from keras.layers.wrappers import Bidirectional
from keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint
from keras import Input, Model, utils
from keras.preprocessing.sequence import pad_sequences
import logging
from keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow.python.keras import backend as K
import subprocess
from text_vectorian import SentencePieceVectorian
import pickle

logger = logging.getLogger(__name__)
vectorian = SentencePieceVectorian()

def _run_prepare_commands():
    '''
    äº‹å‰æº–å‚™ã¨ã—ã¦OSã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    '''

    commands = '''
pip freeze
'''

    for command in commands.split('\n'):
        if command == '':
            continue
        ret = subprocess.call(command.split(' '))
        logger.info(ret)

def _save_model(model, history, model_dir, output_dir):
    '''
    ãƒ¢ãƒ‡ãƒ«ã‚„å®Ÿè¡Œå±¥æ­´ã‚’ä¿å­˜ã—ã¾ã™ã€‚
    '''

    model_filename = model_dir + '/model_v1.h5'

    logger.info(f'Model save as {model_filename}.')
    model.save(model_filename)

    if isinstance(history, keras.callbacks.History):
        history_df = pd.DataFrame(history.history)

    history_df.to_csv(output_dir + '/history.csv')

def _save_labels(data, output_dir):
    '''
    æ¨è«–ã§åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚
    '''

    del data['labels']
    del data['features']

    data_filename = output_dir + '/labels.pickle'

    with open(data_filename, mode='wb')as f:
        pickle.dump(data, f)

def _load_labeldata(train_dir):
    '''
    ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    '''

    features_df = pd.read_csv(f'{train_dir}/features.csv')
    labels_df = pd.read_csv(f'{train_dir}/labels.csv')
    label2index = {k: i for i, k in enumerate(labels_df['label'].unique())}
    index2label = {i: k for i, k in enumerate(labels_df['label'].unique())}
    class_count = len(label2index)
    labels = utils.np_utils.to_categorical([label2index[label] for label in labels_df['label']], num_classes=class_count)

    features = []

    for feature in features_df['feature']:
        features.append(vectorian.fit(feature).indices)

    features = pad_sequences(features, maxlen=vectorian.max_tokens_len)

    logger.info(f'ãƒ‡ãƒ¼ã‚¿æ•°: {len(features_df)}, ãƒ©ãƒ™ãƒ«æ•°: {class_count}, Labels Shape: {labels.shape}, Features Shape: {features.shape}')

    return {
        'class_count': class_count,
        'label2index': label2index,
        'index2label': index2label,
        'labels': labels,
        'features': features,
        'input_len': vectorian.max_tokens_len
    }

def _create_model(input_shape, hidden, dropout, class_count):
    '''
    ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’å®šç¾©ã—ã¾ã™ã€‚
    '''

    input_tensor = Input(input_shape)
    common_input = vectorian.get_keras_layer(trainable=True)(input_tensor)
    x1 = Bidirectional(LSTM(hidden))(common_input)
    x1 = Dense(hidden * 2, activation='relu')(x1)
    x1 = Dropout(dropout)(x1)
    output_tensor = Dense(class_count, activation='softmax')(x1)

    model = Model(input_tensor, output_tensor)
    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc', 'mse', 'mae'])
    model.summary()

    return model

if __name__ == '__main__':
    # Estimatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å¼•æ•°ã¨ã—ã¦æŒ‡å®šã•ã‚Œã‚‹ãŸã‚ã€argparseã§è§£æã—ã¾ã™ã€‚
    # Pythonã®æ–‡æ³•ä¸ŠEstimatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ã§ã™ãŒã€å¼•æ•°ã¨ã—ã¦æ¸¡ã•ã‚Œã‚‹å ´åˆã¯`--`ã‚’ãƒ—ãƒ¬ãƒ•ã‚£ã‚¯ã‚¹ã«ã—ãŸãƒã‚§ãƒ¼ãƒ³ã‚±ãƒ¼ã‚¹ã«ãªã£ã¦ã„ã¾ã™ã€‚
    # ãªãŠhyperparameterã¨ã—ã¦æ¸¡ã™dictã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€åå‰ã®å¤‰æ›ã¯è¡Œã‚ã‚Œãªã„ã®ã§ãã®ã¾ã¾å¼•æ•°åã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™ã€‚
    parser = argparse.ArgumentParser()
    # Estimatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ¸¡ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=100)
    parser.add_argument('--hidden', type=int)
    parser.add_argument('--dropout', type=float)
    parser.add_argument('--container-log-level', type=int, default=logging.INFO)
    # ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦æ¸¡ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    parser.add_argument('--test-dir', type=str, default=os.environ['SM_CHANNEL_TEST'])
    parser.add_argument('--output-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])

    args, _ = parser.parse_known_args()
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’å¼•æ•°ã§æ¸¡ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒŠã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã¨åˆã‚ã›ã¾ã™ã€‚
    logging.basicConfig(level=args.container_log_level)

    # äº‹å‰æº–å‚™ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å®Ÿè¡Œã¯ä»»æ„ã§ã™ã€‚
    _run_prepare_commands()

    # ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    data = _load_labeldata(args.train_dir)
    # ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã‚’è¡Œã„ã¾ã™ã€‚
    model = _create_model(data['features'][0].shape, args.hidden, args.dropout, data['class_count'])
    # å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚
    train_features = data['features']
    train_labels = data['labels']
    # å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    # verboseã‚’ã€€2ã«æŒ‡å®šã™ã‚‹ã®ã¯ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã§ã™ãŒã€ãã®ã¾ã¾ã§ã¯ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒãƒ¼ã®å‡ºåŠ›æ¯ã«ãƒ­ã‚°ãŒè¨˜éŒ²ã•ã‚Œã‚‹ãŸã‚å†—é•·ã§ã™ã€‚
    # 2äºŒã™ã‚‹ã“ã¨ã§ã€epochã”ã¨ã®çµæœã ã‘å‡ºåŠ›ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
    history = model.fit(train_features, train_labels,
        batch_size=args.batch_size,
        validation_split=0.1,
        epochs=args.epochs,
        verbose=2,
        callbacks = [
            EarlyStopping(patience=5, monitor='val_acc', mode='max'),
        ])

    # å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã€‚
    _save_model(model, history, args.model_dir, args.output_dir)
    # æ¨è«–æ™‚ã«åˆ©ç”¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚
    _save_labels(data, args.output_dir)
```

## å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

ä»¥é™ã¯ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å®Ÿè¡Œã—ã¾ã™ã€‚

ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆã¨ãªã‚‹S3ã‚„ã‚¸ãƒ§ãƒ–å®Ÿè¡Œåã¨ãªã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãªã©ã®æƒ…å ±ã§ã™ã€‚
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¯ã«å®šç¾©ã—ã¾ã™ã€‚

```python
import sys
sys.path.append('jobs/modules')

TAGS = [{ 'Key': 'example.com:example.ProjectName', 'Value': 'knbc' }]
PROJECT_NAME = 'knbc'
VERSION = 'v1'
S3_BUCKET = 's3://sagemaker-us-east-1.example.com/knbc'
TRAINS_DIR = S3_BUCKET + '/data/trains'
TESTS_DIR = S3_BUCKET + '/data/tests'
OUTPUTS_DIR = S3_BUCKET + '/outputs'
```

## Estimatorã®æº–å‚™

`Estimator`ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ã‚’èª­ã¿è¾¼ã‚“ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’ã©ã®ã‚ˆã†ã«å®Ÿè¡Œã™ã‚‹ã‹ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©

`metric_definitions`ã«æ¸¡ã™dictã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡ŒçŠ¶æ³ã¯CloudWatchã®ãƒ­ã‚°ã¨ã—ã¦å‡ºåŠ›ã•ã‚Œã¾ã™ãŒã€ã“ã®å®šç¾©ã«å¾“ã£ã¦å­¦ç¿’çµŒéã®ãƒ­ã‚°ãŒè§£æã•ã‚ŒCloudWatch Logsã§è¦–è¦šåŒ–ã•ã‚Œã¾ã™ã€‚

`Name`ãŒCloudWatchã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã€`Regex`ãŒãƒ­ã‚°ã‚’ã©ã®ã‚ˆã†ã«è§£æã™ã‚‹ã‹ã‚’æ­£è¦è¡¨ç¾ã§æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚`(\S+)`ãŒæ•°å€¤ã‚’æŒ‡ã—ç¤ºã™äº‹ã§ã€ãã®å†…å®¹ãŒã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã•ã‚Œã‚‹ã¨è€ƒãˆã¦ãã ã•ã„ã€‚

### TensorFlowã«è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

#### role

S3ã‹ã‚‰ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ãŸã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚„å‡ºåŠ›çµæœã‚’é…ç½®ã™ã‚‹ãŸã‚ã®æ¨©é™ã‚’è¡¨ã™IAM Roleã§ã™ã€‚

#### script_mode

`True`ã‚’æŒ‡å®šã—ã¾ã™ã€‚

#### dependencies

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é…ç½®ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¾ã™ã€‚

#### metric_definitions

ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©ã‚’æŒ‡å®šã—ã¾ã™ã€‚æŒ‡å®šã—ãªã„ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦ä½•ã‚‚è¨˜éŒ²ã•ã‚Œãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´ã‚‚ã§ãã¾ã›ã‚“ã€‚

```python
from sagemaker.tensorflow import TensorFlow
import logging

params = {
    'batch-size': 512,
    'epochs': 10,
    'hidden': 32,
    'dropout': 0.1
}
metric_definitions = [
    {'Name': 'train:acc', 'Regex': 'acc: (\S+)'},
    {'Name': 'train:mse', 'Regex': 'mean_squared_error: (\S+)'},
    {'Name': 'train:mae', 'Regex': 'mean_absolute_error: (\S+)'},
    {'Name': 'valid:acc', 'Regex': 'val_acc: (\S+)'},
    {'Name': 'valid:mse', 'Regex': 'val_mean_squared_error: (\S+)'},
    {'Name': 'valid:mae', 'Regex': 'val_mean_absolute_error: (\S+)'},
]
estimator = TensorFlow(
    entry_point='jobs/train.py',
    role='arn:aws:iam::1234567890123:role/service-role/AmazonSageMaker-ExecutionRole-20181129T043923',
    train_instance_count=1,
    train_instance_type='ml.p2.xlarge',
    framework_version='1.12.0',
    py_version='py3',
    script_mode=True,
    hyperparameters=params,
    output_path=OUTPUTS_DIR,
    dependencies=['jobs/modules'],
    container_log_level=logging.INFO,
    metric_definitions=metric_definitions,
    tags=TAGS
)
inputs = {'train': TRAINS_DIR, 'test': TESTS_DIR}
```

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ

Estimatorã®fitãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜

#### job_name

ã‚¸ãƒ§ãƒ–åã¯ä¸€æ„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚è‡ªå‹•çš„ã«ä¸€æ„ã«ã¯ã—ã¦ãã‚Œãªã„ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„uuidã‚’è¿½åŠ ã™ã‚‹ã®ãŒãŠå‹§ã‚ã§ã™ã€‚

#### inputs

ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¾ã™ã€‚å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ãªã©ã‚’åˆ¥ã‘ã¦æŒ‡å®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

```python
import shortuuid

uuid = shortuuid.ShortUUID().random(length=8)
estimator.fit(job_name=f'{PROJECT_NAME}-{VERSION}-s-{uuid}', inputs=inputs)
```

```
    [31m - 4s - loss: 1.3353 - acc: 0.3528 - mean_squared_error: 0.1818 - mean_absolute_error: 0.3587 - val_loss: 1.2979 - val_acc: 0.4164 - val_mean_squared_error: 0.1763 - val_mean_absolute_error: 0.3561[0m
    [31mEpoch 2/10[0m
    [31m - 2s - loss: 1.2878 - acc: 0.3950 - mean_squared_error: 0.1757 - mean_absolute_error: 0.3540 - val_loss: 1.2628 - val_acc: 0.4297 - val_mean_squared_error: 0.1719 - val_mean_absolute_error: 0.3455[0m
    [31mEpoch 3/10[0m
    [31m - 2s - loss: 1.2295 - acc: 0.4507 - mean_squared_error: 0.1678 - mean_absolute_error: 0.3422 - val_loss: 1.1847 - val_acc: 0.4721 - val_mean_squared_error: 0.1617 - val_mean_absolute_error: 0.3242[0m
    [31mEpoch 4/10[0m
    [31m - 2s - loss: 1.1362 - acc: 0.5307 - mean_squared_error: 0.1543 - mean_absolute_error: 0.3207 - val_loss: 1.1010 - val_acc: 0.5544 - val_mean_squared_error: 0.1486 - val_mean_absolute_error: 0.3119[0m
    [31mEpoch 5/10[0m
    [31m - 2s - loss: 1.0099 - acc: 0.6000 - mean_squared_error: 0.1356 - mean_absolute_error: 0.2897 - val_loss: 1.0430 - val_acc: 0.5252 - val_mean_squared_error: 0.1440 - val_mean_absolute_error: 0.2795[0m
    [31mEpoch 6/10[0m
    [31m - 2s - loss: 0.9278 - acc: 0.6372 - mean_squared_error: 0.1240 - mean_absolute_error: 0.2628 - val_loss: 0.9230 - val_acc: 0.6207 - val_mean_squared_error: 0.1248 - val_mean_absolute_error: 0.2580[0m
    [31mEpoch 7/10[0m
    [31m - 2s - loss: 0.8285 - acc: 0.6817 - mean_squared_error: 0.1103 - mean_absolute_error: 0.2356 - val_loss: 0.8919 - val_acc: 0.6313 - val_mean_squared_error: 0.1201 - val_mean_absolute_error: 0.2460[0m
    [31mEpoch 8/10[0m
    [31m - 2s - loss: 0.7441 - acc: 0.7227 - mean_squared_error: 0.0978 - mean_absolute_error: 0.2134 - val_loss: 0.8506 - val_acc: 0.6631 - val_mean_squared_error: 0.1125 - val_mean_absolute_error: 0.2268[0m
    [31mEpoch 9/10[0m
    [31m - 2s - loss: 0.6586 - acc: 0.7611 - mean_squared_error: 0.0861 - mean_absolute_error: 0.1893 - val_loss: 0.8458 - val_acc: 0.6711 - val_mean_squared_error: 0.1113 - val_mean_absolute_error: 0.2179[0m
    [31mEpoch 10/10[0m
```

## HyperparameterTunerã®æº–å‚™

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ã‚’ã©ã®ã‚ˆã†ã«è¡Œã†ã‹ã‚’`HyperparameterTuner`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å®šç¾©ã—ã¾ã™ã€‚
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯`Bayesian`ã®ã¿ã§ã™ãŒã€å°†æ¥çš„ã«ã¯å¢—ãˆã‚‹ã¨æ€ã„ã¾ã™ã€‚

### æœ€é©åŒ–ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©

`Estimator`ã§æŒ‡å®šã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸­ã‹ã‚‰é¸æŠã—ã¾ã™ã€‚æŒ‡å®šã™ã‚‹ã®ã¯`Name`ã«æŒ‡å®šã—ãŸæ–‡å­—åˆ—ã§ã™ã€‚
æŒ‡å®šã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æœ€å¤§åŒ–ã‚’è¡Œã†å ´åˆã¯`Maximaize`ã€æœ€å°åŒ–ã‚’è¡Œã†å ´åˆã¯`Minimize`ã‚’æŒ‡å®šã—ã¾ã™ã€‚

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´å¹…ã®å®šç¾©

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ã‚’è¡Œã†å†…å®¹ã§ã™ã€‚
ã“ã“ã§æŒ‡å®šã—ãŸèª¿æ•´å¹…å†…ã®å€¤ãŒ`Estimator`ã‚’çµŒç”±ã—ã¦`ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ`ã«æ¸¡ã•ã‚Œã¾ã™ã€‚

`ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ`ã§ã¯å¼•æ•°ã¨ã—ã¦æ¸¡ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å†…å®¹ã‚’åˆ©ç”¨ã—ãŸå­¦ç¿’ã‚’è¡Œã„ã€
ãã®çµæœã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦è©•ä¾¡ã™ã‚‹äº‹ã§ã€æœ€é©åŒ–ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæœ€å°ã‚ã‚‹ã„ã¯æœ€å¤§ã¨ãªã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜

#### objective_metric_name

æœ€é©åŒ–ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã™ã€‚

#### hyperparameter_ranges

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´å¹…ã®å®šç¾©ã§ã™ã€‚dictã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æŒ‡å®šã—ã¾ã™ã€‚

#### early_stopping_type

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¸Šã€ã“ã‚Œä»¥ä¸Šæœ€é©åŒ–ãŒå›°é›£ã¨åˆ¤æ–­ã—ãŸæ®µéšã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´ã‚¸ãƒ§ãƒ–ã‚’åœæ­¢ã—ã¾ã™ã€‚
`Auto`ã«ã—ã¦ã„ã‚‹å ´åˆã¯ã€å¾Œè¿°ã®`max_jobs`æœªæº€ã§æ¢ç´¢ãŒçµ‚äº†ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

#### max_jobs

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã§å®Ÿè¡Œã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å»¶ã¹æ•°ã®ä¸Šé™ã§ã™ã€‚

```python
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner

objective_metric_name = 'valid:mse'
objective_type = 'Minimize'
hyperparameter_ranges = {
    'hidden': IntegerParameter(32, 512),
    'dropout': ContinuousParameter(0.1, 0.7)
}

tuner = HyperparameterTuner(estimator,
                            objective_metric_name,
                            hyperparameter_ranges,
                            metric_definitions,
                            max_jobs=50,
                            max_parallel_jobs=1,
                            objective_type=objective_type,
                            early_stopping_type='Auto',
                            tags=TAGS)
```

## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´ã®å®Ÿè¡Œ

HyperparameterTunerã®fitã«ã‚ˆã‚Šè‡ªå‹•èª¿æ•´ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œã¨åŒã˜æ§˜ãªæ„Ÿã˜ã§ã™ã€‚

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜

#### job_name

ã‚¸ãƒ§ãƒ–åã¯ä¸€æ„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚è‡ªå‹•çš„ã«ä¸€æ„ã«ã¯ã—ã¦ãã‚Œãªã„ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„uuidã‚’è¿½åŠ ã™ã‚‹ã®ãŒãŠå‹§ã‚ã§ã™ã€‚
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•èª¿æ•´ã¯æ¨æ¸¬ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§æŒ‡å®šã—ãŸåå‰ã®ãƒã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ã¨ã—ã¦ã•ã‚‰ã«æ–‡å­—åˆ—ã‚’è¿½åŠ ã—ã¾ã™ã€‚
ã“ã®ãŸã‚ã€æœ€å¤§32æ–‡å­—ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®`job_name`ã‚ˆã‚Šã‚‚åˆ¶é™ãŒå³ã—ããªã£ã¦ã„ã¾ã™ã€‚

#### inputs

ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¾ã™ã€‚å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ãªã©ã‚’åˆ¥ã‘ã¦æŒ‡å®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

```python
import shortuuid

uuid = shortuuid.ShortUUID().random(length=3)
tuner_job_name = f'{PROJECT_NAME}-{VERSION}-o-{uuid}'
tuner.fit(job_name=tuner_job_name, inputs=inputs)
```

## è‡ªå‹•èª¿æ•´çµæœã®ç¢ºèª

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ã¯é•·ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ã“ã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§éåŒæœŸå®Ÿè¡Œã«ãªã£ã¦ã„ã¾ã™ã€‚
AWSã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãªã©ã§ç¢ºèªã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ãŒå®Œäº†ã—ãŸã‚‰ã€å¯¾è±¡ã®ã‚¸ãƒ§ãƒ–åã‚’è¨˜éŒ²ã—ã¦ä¸‹ã•ã„ã€‚

`HyperparameterTuner.attach`é–¢æ•°ã«ã‚¸ãƒ§ãƒ–å(job_name)ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢çµæœã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner
import seaborn as sns

job_name = tuner_job_name
tuner = HyperparameterTuner.attach(job_name)
df = tuner.analytics().dataframe().drop(['TrainingEndTime', 'TrainingJobName', 'TrainingJobStatus', 'TrainingStartTime'], axis=1).dropna()

sns.pairplot(df, kind='reg')
```

![png](images/output_16_2.png)

## æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

```python
import keras
import boto3
import pickle
from urllib.parse import urlparse

estimator = TensorFlow.attach(tuner.best_training_job())
print(tuner.best_training_job())

url = urlparse(estimator.model_data)
s3_root_dir = '/'.join(url.path.split('/')[:-2])[1:]
model_s3path = s3_root_dir + '/output/model.tar.gz'
output_s3path = s3_root_dir + '/output/output.tar.gz'
model_filename = 'models/model_v1.h5'
s3 = boto3.resource('s3')
bucket = s3.Bucket(url.netloc)

print(model_s3path)
bucket.download_file(model_s3path, 'models/model.tar.gz')
bucket.download_file(output_s3path, 'models/output.tar.gz')
!cd models; tar zxvf model.tar.gz; tar zxvf output.tar.gz

model = keras.models.load_model(model_filename)

with open('models/labels.pickle', mode='rb') as f:
    labels = pickle.load(f)
```

## æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«–çµæœç¢ºèª

```python
from text_vectorian import SentencePieceVectorian
from keras.preprocessing.sequence import pad_sequences
import pandas as pd

vectorian = SentencePieceVectorian()
tests_features_df = pd.read_csv(f'{TESTS_DIR}/features.csv')
tests_labels_df = pd.read_csv(f'{TESTS_DIR}/labels.csv')
tests_features = []
for feature in tests_features_df['feature']:
    tests_features.append(vectorian.fit(feature).indices)
tests_features = pad_sequences(tests_features, maxlen=labels['input_len'])
```

```python
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import keras

predicted_labels = model.predict(tests_features).argmax(axis=1)
numeric_labels = [labels['label2index'][label] for label in tests_labels_df['label']]
print(classification_report(numeric_labels, predicted_labels, target_names=labels['index2label'].values()))
```

```
                 precision    recall  f1-score   support

             äº¬éƒ½       0.70      0.77      0.74       137
           æºå¸¯é›»è©±       0.81      0.74      0.77       145
           ã‚¹ãƒãƒ¼ãƒ„       0.72      0.72      0.72        47
            ã‚°ãƒ«ãƒ¡       0.75      0.74      0.75        90

    avg / total       0.75      0.75      0.75       419
```

## å‚è€ƒæ–‡çŒ®

* [Keras ã§Amazon SageMaker ã‚’ä½¿ç”¨ã™ã‚‹ï¼ˆScript Modeï¼‰](https://qiita.com/rtaguchi/items/8422b9802ec7c4e75215)
* [TensorFlow SageMaker Estimators and Models](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#preparing-a-script-mode-training-script)
* [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/latest/index.html)
